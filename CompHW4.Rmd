---
title: "Decision Trees, Random Forests and Clustering"
author: "Nina Groleger"
date: "23 11 2020"
output:
  pdf_document: default
  html_document: default
---

```{r, message = FALSE}
library(tidyverse)
library(rpart)
#install.packages("caret") if necessary
library(caret)
library(randomForest)
```

# Part 1: Decision Trees
```{r}
income <- read_csv("income.csv", col_types = "nffnfffffnff")
summary(income)
```

## Problem 1. 
Using the income dataset, perform 5-fold cross validation to find a good value of depth(you should try several values of depth).


We're using 5-fold cross validation here, which means we divide D into 5 parts using random samples, interchange which (1) part is Dtest, the rest are Dtrain. This is a way to "fake" having more data than we do. Then we compute the average error so 1/5sum(erri)
```{r p1}
test_sets <- createFolds(income$income, k=5)
err_matrix <- matrix(0, 5, 5)

for (j in 1:5){
  for (i in 1:5){
    test_idx <- unlist(test_sets[i], use.names=FALSE)
    cv_train <- income[test_idx,]
    cv_test <- income[-test_idx,]
  
    # Make models with cv_train
    treetrain <- rpart(income ~ ., data = cv_train, method="class", control = rpart.control(maxdepth=(j*2), cp=0))
  
    # Make predictions with cv_test
    treetest <- round(predict(treetrain, cv_test))
  
    # Calculate error
    testing_col <- c(cv_test$income == "<=50K")
    calc <- data.frame(real = testing_col, pred = treetest[,1]) %>% mutate(accurate = (real==pred))
    err_matrix[j, i] <- 1 - sum(calc$accurate)/length(calc$accurate)
  }
}

depth_errors <- data.frame(err_matrix) %>% mutate(average_err = rowMeans(err_matrix), depth = c(2, 4, 6, 8, 10))
depth_errors %>% ggplot(aes(x=depth, y=average_err)) +
  geom_line()
```

Our lowest error is for depth 6. 

## Problem 2.
Using the value of depth found in Problem 1, fit a decision tree with this depth to the entire dataset, and visualize the tree. 

Using this plot, predict the income level for a person with the following characteristics:
* 45 years old
* Privately employed in sales
* Bachelors degree w/ 13 years of education
* White
* 40 working hours
* From the US
* Woman
* Married

Explain in words how your model makes this prediction.

```{r p2}
tree_model <- rpart(income ~., income, method="class", control = rpart.control(maxdepth = 6))
plot(tree_model, margin = 0.1, uniform = TRUE)
text(tree_model, fancy = FALSE, use.n = TRUE, all = TRUE)
tree_model
```

We follow the splits from the root node to a leaf. Our person is married so we follow node 3, then the education level: Bachelors, to node 7. At this point we don't need to check any other splits because we've reached a leaf. We predict the income level for our person is above 50k with 72% probability.

# Part 2: Random forests

## Problem 3: Finding the number of trees
Split the income data into a training and a testing set. Then, fit random forest models on the training data for a range of different values of number of trees. For each model, compute the test error, and plot it against the number of trees in the random forest. Interpret the results.

```{r p3}
idx <- sample(c(1:nrow(income)), round(0.8*nrow(income)), replace = FALSE)
Dtrain <- income[idx,]
Dtest <- income[-idx,]
forest_err <- rep(0, 20)

for(i in 1:50){
  forest <- randomForest(income ~., Dtrain, ntree=i)
  prediction2 <- predict(forest, Dtest)
  
  # Calculate error
  calc <- data.frame(real = Dtest$income, pred = prediction2) %>% mutate(accurate = (real==pred))
  forest_err[i] <- 1 - sum(calc$accurate)/length(calc$accurate)
}

err_frame <- data.frame(test_error = forest_err, trees = c(1:50))
err_frame %>% ggplot(aes(x=trees, y=test_error)) +
  geom_line()
```

While the error is a little jittery, in general the error decreases with an increasing number of trees. This makes sense, as a single tree has high variance and therefore a higher test error than we would like. By using multiple trees, we avoid this high variance, and achieve a lower testing error. 

## Problem 4.Based on your results from Problem 3, fit a random forest model to the income data with a ‘good’ number of trees. 
Then, inspect the feature importances. Which feature is the most important inpredicting income? Which feature is the least important? How does this compare with the interpretation ofthe decision tree model in Part 1?

```{r p4}
# fit forest
forest2 <- randomForest(income ~., income, ntree=15, importance = TRUE)
forest2$importance
```
We look at mean decrease in accuracy: The most important feature (in this run) with respect to accuracy is marital status, and the least important feature is native country. 

# Part 3: Clustering
```{r, message = FALSE}
college <- read_csv("college.csv", col_types = "nccfffffnnnnnnnnn")
summary(college)
```

## Problem 5.
* First, perform the following: create a new dataset consisting of colleges only in the state of Maryland, and keep only the features admission_rate and sat_avg. 
* Then, scale the dataset, and perform K-means clustering with K= 1,2, . . . ,10 using the Euclidean distance (you can either use your implementation of K-means that you wrote in lab, or an imported R/Python function). 
* Then, compute the within-cluster sum of squared distances (WSSD) for each value of K, and plot it against K. Using the elbow method, what do you think is a good value of K for this problem? 
* Visualize the clusters at this value of K, and interpret the results.

```{r p5}
ma <- college %>% filter(state == "MD") %>% select(admission_rate, sat_avg)
ma_scaled <- scale(ma)
km_df <- data.frame(k = c(1:10), wssd = rep(0, 10))
for (i in 1:10){
  km_df$wssd[i] <- kmeans(ma_scaled, i)$tot.withinss
}

km_df %>% ggplot(aes(x=k, y=wssd)) +
  geom_line()
```

Looking at the "elbow" of the graph, we pick k=4.

```{r visualize}
# Visualize the clusters at this value of K, and interpret the results.
k4_all <- kmeans(ma_scaled, 4)
data_clusters <- data.frame(ma_scaled, cluster = as.factor(k4_all$cluster))
ggplot(data_clusters, aes(x=admission_rate, y=sat_avg)) +
  geom_point(aes(color=cluster))
```
The difference between k = 3 and k = 4 is really that top left result, which is very different from the rest. With k = 4, we can cluster the remaining data into 3 categories. It seems the clusters are (1) low sat_avg, (2) medium sat_avg with medium admission_rate and (3) medium sat_avg with high admission_rate. 

## Problem 6.
Create a new dataset consisting of colleges in Maryland, and keep the features admission_rate,sat_avg and control. Scale the continuous features admission_rate and sat_avg. Using this dataset,compute the distance matrix D as defined in the hw doc, and use it to perform heirarchical clustering. Compare the clusters you obtain to those obtained in Problem 5 with K-means clustering.

```{r prep}
sat_adm <- ma_scaled
control <- college %>% filter(state == "MD") %>% select(control)
ma_df <- data.frame(sat_adm, control)
```

```{r distance}
# We make our distance matrix, the (i, j)th entry is d(xi, xj)
dist_mat <- matrix(0, nrow(ma_df), nrow(ma_df))
for (i in 1:nrow(ma_df)){
  for (j in 1:nrow(ma_df)){
    point_dist <- sqrt((ma_df$admission_rate[i] - ma_df$admission_rate[j])^2 +
      (ma_df$sat_avg[i] - ma_df$sat_avg[j])^2)
    if (ma_df$control[i] != ma_df$control[j]){
      point_dist <- point_dist + 1
    }
    dist_mat[i, j] <- point_dist
  }
}
```

```{r clustering}
dist_d <- as.dist(dist_mat)
clusters <- hclust(dist_d)
plot(clusters)
```

These are comparable to our problem 5 visualization. Note that 1 is very separate from the rest of our points - this is the point on the left upper side of the graph. In fact, we can see all of our four clusters in this visualization but it's also easier to see what clustering would look like with a lower or higher k. However, this kind of visualization doesn't tell us which features influenced the clustering more so we can't say how much the control of the college influences the clustering. 
